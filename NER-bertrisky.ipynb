{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for Named Entity Recognition\n",
    "\n",
    "### This notebook builds off the official BERT Github, and draws inspiration from;\n",
    "#### https://towardsdatascience.com/building-a-multi-label-text-classifier-using-bert-and-tensorflow-f188e0ecdc5d\n",
    "#### https://github.com/kyzhouhzau/BERT-NER \n",
    "\n",
    "### Before getting started you will want to go and download your choice of BERT base model from https://github.com/google-research/bert\n",
    "For this notebook we use Bert-Base, Cased\n",
    "If you can fit Bert-Large into GPU memory then I congratulate you as you are a very wealthy person.\n",
    "The zip file of the model will six files in it and your model will need them all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HARDWARE REQUIREMENTS #### \n",
    "I trained the model on 4 x Tesla 16 GB GPUs + 52 GB ram for speed (20 minutes or so). When serving the pretrained model using a checkpoint already pretrained on the NER data you can just use regular RAM and probably can get away with 32 GB of regular RAM and no GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl\n",
      "Collecting six (from bert-tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six, bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1 six-1.12.0\n"
     ]
    }
   ],
   "source": [
    "##install bert if not already done\n",
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 03:28:31.596731 139806183552768 deprecation_wrapper.py:119] From /home/jupyter/.local/lib/python3.5/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import modeling\n",
    "from bert import tokenization\n",
    "from bert.tokenization import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Model\n",
    "# Initialize session\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##use downloaded BERT model, change path accordingly\n",
    "BERT_VOCAB= 'BERTcased/vocab.txt'\n",
    "BERT_INIT_CHKPNT = 'BERTcased/bert_model.ckpt'\n",
    "BERT_CONFIG = 'BERTcased/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are two ways to create the special tokenizer that BERT requires\n",
    "##1. Use the hub_module as below (online)\n",
    "bert_path = \"https://tfhub.dev/google/bert_cased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=False)\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "##2. Create from the downloaded vocab.txt and init chkpnt file as below (for offline on prem)\n",
    "#tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
    "#tokenizer = tokenization.FullTokenizer(\n",
    "#      vocab_file=BERT_VOCAB, do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'B',\n",
       " '##ER',\n",
       " '##T',\n",
       " 'token',\n",
       " '##izer',\n",
       " '.',\n",
       " 'Like',\n",
       " 'it',\n",
       " 'very',\n",
       " 'much',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test our tokenizer is working\n",
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer. Like it very much.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS Tag\n",
       "0  Sentence: 1      Thousands  NNS   O\n",
       "1          NaN             of   IN   O\n",
       "2          NaN  demonstrators  NNS   O\n",
       "3          NaN           have  VBP   O\n",
       "4          NaN        marched  VBN   O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##IMPORT DATA\n",
    "##change path accordingly\n",
    "data = pd.read_csv('datasets/ner_dataset.csv', encoding=\"ISO-8859-1\")\n",
    "data.head()\n",
    "#Below isn't the end dataset format we want, see block 11 output for the optimal dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-geo', 'B-gpe', 'B-per', 'I-geo', 'B-org', 'I-org', 'B-tim', 'B-art', 'I-art', 'I-per', 'I-gpe', 'I-tim', 'B-nat', 'B-eve', 'I-eve', 'I-nat']\n"
     ]
    }
   ],
   "source": [
    "#GET a list of all the possible labels for our entities by aggregating the label(tag) column\n",
    "s = data['Tag']\n",
    "tag_labels = s.unique().tolist()\n",
    "print(tag_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "ID = 'id'\n",
    "DATA_COLUMN = 'Word'\n",
    "LABEL_COLUMNS = ['Tag']\n",
    "#account for special tags that will be added by BERT tokenizer\n",
    "bert_tags = [\"X\",\"[CLS]\",\"[SEP]\"]\n",
    "padding_tag = [\"[PAD]\",]\n",
    "#below are the two important variables, we will reference these a lot.\n",
    "LABELS = padding_tag + tag_labels + bert_tags\n",
    "num_labels = len(LABELS)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-nat': 17, 'B-eve': 15, 'O': 1, 'I-eve': 16, '[PAD]': 0, '[SEP]': 20, 'I-org': 7, 'I-art': 10, 'B-geo': 2, 'X': 18, 'B-nat': 14, 'I-gpe': 12, 'B-gpe': 3, 'I-geo': 5, 'B-org': 6, 'B-art': 9, 'I-per': 11, 'I-tim': 13, 'B-per': 4, '[CLS]': 19, 'B-tim': 8}\n"
     ]
    }
   ],
   "source": [
    "#Create an dictionary for our labels so we can replace them with integers (for our neural net) and look them up later.\n",
    "label_map = {}\n",
    "for (i,label) in enumerate(LABELS):\n",
    "  label_map[label] = i\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 1</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10</th>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 100</th>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 1000</th>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10000</th>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Word  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [Thousands, of, demonstrators, have, marched, ...   \n",
       "Sentence: 10     [Iranian, officials, say, they, expect, to, ge...   \n",
       "Sentence: 100    [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "Sentence: 1000   [They, left, after, a, tense, hour-long, stand...   \n",
       "Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                               POS  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "Sentence: 10     [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "Sentence: 100    [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "Sentence: 1000      [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "Sentence: 10000  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                               Tag  \n",
       "Sentence #                                                          \n",
       "Sentence: 1      [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...  \n",
       "Sentence: 10     [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...  \n",
       "Sentence: 100    [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...  \n",
       "Sentence: 1000                   [O, O, O, O, O, O, O, O, O, O, O]  \n",
       "Sentence: 10000  [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This particular dataset has blanks in the sentence column like a pivot table, so we need to fill them in\n",
    "#Note that the new dataframe is reordered alphabetically (we aren't missing rows)\n",
    "#fill in the blanks for Sentence\n",
    "data.loc[:,'Sentence #'].fillna(method='ffill', inplace = True)\n",
    "#group each sentence and make arrays of other columns\n",
    "data = data.groupby('Sentence #').agg(list)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARD BERT DEFINITIONS as per BERT github\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True, is_predict=False):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids,\n",
    "        self.is_real_example=is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long. Increasing this will hit your memory very hard!\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARD BERT DEFINITIONS with slight tweak\n",
    "def create_examples(df, labels_available=True, pretokenized=False):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    #process dataframe to produce individual input examples where text_a is an array of words and labels is an array of NER-Tag labels\n",
    "    for (i, row) in tqdm(enumerate(df.values)):\n",
    "        guid = i\n",
    "        #grab your input text that you wish to label\n",
    "        #your input text is assumed to be in word tokenized comma-seperated form ['this','is','my','sentence']\n",
    "        text_a = row[0]\n",
    "        if labels_available:\n",
    "            #Grab whatever column has your array of target labels ['I','O','I']\n",
    "            labels = row[2]\n",
    "        else:\n",
    "            if pretokenized:\n",
    "                labels = row[1]\n",
    "            else:\n",
    "                labels = ['[PAD]']*len(row[0])\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE DATA FOR TRAINING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43163it [00:00, 511852.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#SPLIT TRAINING AND TEST DATA\n",
    "TRAIN_VAL_RATIO = 0.9\n",
    "LEN = data.shape[0]\n",
    "SIZE_TRAIN = int(TRAIN_VAL_RATIO*LEN)\n",
    "\n",
    "x_train = data[:SIZE_TRAIN]\n",
    "x_val = data[SIZE_TRAIN:]\n",
    "\n",
    "train_examples = create_examples(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STANDARD BERT DEFINITIONS - with tweaks\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def convert_single_example(ex_index, example, max_seq_length,\n",
    "                           tokenizer, labelmap = label_map, pretokenized=False):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        #we add this 'org_to_tok_map' to eventually keep track of which input word relates to what output label\n",
    "        #note this gets messy as BERT will split some words like 'running' into 'run' '##ing'\n",
    "        orig_to_tok_map = [0,]\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_ids=[0] * max_seq_length,\n",
    "            is_real_example=False), orgin_to_tok_map\n",
    "    \n",
    "    orig_to_tok_map = []\n",
    "    tokens_a = []\n",
    "    label_ids = []\n",
    "    Xindex = labelmap['X']\n",
    "    #becuase the NER dataset is already split by word, we are going to tokenize one word at a time\n",
    "    #however, remember each word has a Tag, and some of these words might be split 'run','#ing' by BERT tokenizer\n",
    "    #so we need to add sufficient 'X' tags to labellist as we go (Bert uses the 'X' tag to represent the second half of these split words)\n",
    "    \n",
    "    if pretokenized:\n",
    "        tokens_a = example.text_a\n",
    "        finlabels = []\n",
    "        for token in tokens_a:\n",
    "            if token in ['[SEP]','[CLS]','X']:\n",
    "                finlabels.append(int(labelmap[token]))\n",
    "            else:\n",
    "                finlabels.append(0)\n",
    "        label_ids.extend(finlabels) \n",
    "    else:    \n",
    "        for label,word in zip(example.labels,example.text_a):\n",
    "            orig_to_tok_map.append(len(tokens_a)+1)\n",
    "            #convert labels from text ('O','I-geo' etc) to integers\n",
    "            #remember tokenizer returns an array, even if we give it one word\n",
    "            sub_words = tokenizer.tokenize(word)\n",
    "            tokens_a.extend(sub_words)\n",
    "            labelss = np.array([label for x in sub_words])\n",
    "            #add 'X' label for all subword suffixes that BERT produces\n",
    "            labelss[1:] = 'X'\n",
    "            #give first subword component the actual label ('I-org' etc but as an integer)\n",
    "            finlabels = [int(labelmap[tok]) for tok in labelss]\n",
    "            label_ids.extend(finlabels)              \n",
    "    \n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if not pretokenized:\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "                label_ids = label_ids[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    if pretokenized:\n",
    "        tokens = tokens_a\n",
    "    else:\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        label_ids = [labelmap[\"[CLS]\"]] + label_ids + [labelmap[\"[SEP]\"]]\n",
    "    segment_ids = [0] * len(tokens)    \n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        label_ids.append(0)\n",
    "        tokens.append('[PAD]')\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    assert len(label_ids) == max_seq_length\n",
    "    assert len(tokens) == max_seq_length\n",
    "    \n",
    "    if ex_index < 1:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"guid: %s\" % (example.guid))\n",
    "        tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in tokens]))\n",
    "        tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "        tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "        tf.logging.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "        tf.logging.info(\"label: %s (ids = %s)\" % (example.labels, str(label_ids)))    \n",
    "    \n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=label_ids,\n",
    "        is_real_example=True)\n",
    "    \n",
    "    return feature,orig_to_tok_map\n",
    "\n",
    "\n",
    "def file_based_convert_examples_to_features(\n",
    "        examples, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in tqdm(enumerate(examples)):\n",
    "        #if ex_index % 10000 == 0:\n",
    "            #tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature,org_to_tok_map = convert_single_example(ex_index, example,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "            [int(feature.is_real_example)])\n",
    "        if isinstance(feature.label_ids, list):\n",
    "            label_ids = feature.label_ids\n",
    "        else:\n",
    "            label_ids = feature.label_ids[0]\n",
    "        features[\"label_ids\"] = create_int_feature(label_ids)\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "\n",
    "        return example\n",
    "\n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        # For training, we want a lot of parallel reading and shuffling.\n",
    "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "\n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join('./workingBERTNERrisky', \"train.tf_record\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(train_file):\n",
    "    open(train_file, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43163it [00:43, 992.94it/s]\n"
     ]
    }
   ],
   "source": [
    "file_based_convert_examples_to_features(\n",
    "            train_examples, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE MODEL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings, is_predicting=False):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "    \n",
    "    #because we are classifying every word in the sequence we use the below, instead of \n",
    "    #output_layer = model.get_pooled_output() - for example level tagging rather than token tagging\n",
    "    output_layer = model.get_sequence_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            # I.e., 0.1 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        \n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "        if is_predicting:\n",
    "            return(predicted_labels,log_probs)\n",
    "        \n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, predicted_labels, log_probs)\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        #tf.logging.info(\"*** Features ***\")\n",
    "        #for name in sorted(features.keys()):\n",
    "            #tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        is_real_example = None\n",
    "        \n",
    "        if \"is_real_example\" in features:\n",
    "             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "        else:\n",
    "             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "        is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "        \n",
    "        if not is_predicting:   \n",
    "            #CREATE THE MODEL\n",
    "            (loss, predicted_labels, log_probs) = create_model(\n",
    "                bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "                num_labels, use_one_hot_embeddings, is_predicting=False)\n",
    "            #CALCULATE EVALUATION METRICS     \n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names\n",
    "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "\n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "\n",
    "                scaffold_fn = tpu_scaffold\n",
    "            else:\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        #tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            train_op = optimization.create_optimizer(\n",
    "                loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                train_op=train_op,\n",
    "                scaffold=scaffold_fn)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            # Calculate evaluation metrics. \n",
    "            def metric_fn(label_ids, predicted_labels):\n",
    "                accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "                recall = tf.metrics.recall(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                precision = tf.metrics.precision(\n",
    "                    label_ids,\n",
    "                    predicted_labels) \n",
    "                true_pos = tf.metrics.true_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                true_neg = tf.metrics.true_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)   \n",
    "                false_pos = tf.metrics.false_positives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)  \n",
    "                false_neg = tf.metrics.false_negatives(\n",
    "                    label_ids,\n",
    "                    predicted_labels)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"true_positives\": true_pos,\n",
    "                    \"true_negatives\": true_neg,\n",
    "                    \"false_positives\": false_pos,\n",
    "                    \"false_negatives\": false_neg\n",
    "                }\n",
    "\n",
    "            eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=loss,\n",
    "                eval_metric_ops=eval_metrics,\n",
    "                scaffold=scaffold_fn)\n",
    "        else:\n",
    "            (predicted_labels, log_probs) = create_model(bert_config,\n",
    "            is_training, input_ids, input_mask, segment_ids, label_ids, num_labels, use_one_hot_embeddings, is_predicting=True\n",
    "            )       \n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\"probabilities\": log_probs, \"labels\":predicted_labels},\n",
    "                scaffold=scaffold_fn)\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./workingBERTNERrisky/output\"\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    keep_checkpoint_max=1,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  num_labels= num_labels,\n",
    "  init_checkpoint=BERT_INIT_CHKPNT,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=False)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM TRAINING ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0726 03:35:12.763883 139806183552768 estimator.py:360] Skipping training since max_steps has already saved.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "Training took time  0:00:00.155731\n"
     ]
    }
   ],
   "source": [
    "print('Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERFORM EVALUATION ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4796it [00:00, 427970.17it/s]\n",
      "0it [00:00, ?it/s]I0726 03:35:18.140251 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 03:35:18.141622 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 0\n",
      "I0726 03:35:18.142499 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] The four nations on Cha ##vez ' s tour include Argentina , Uruguay , Ecuador , and Bolivia . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 03:35:18.143317 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 1109 1300 6015 1113 24705 21755 112 188 2465 1511 4904 117 11752 117 10244 117 1105 11686 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.144208 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.144945 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.145747 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['O', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'O', 'B-geo', 'O', 'O', 'B-geo', 'O'] (ids = [19, 1, 1, 1, 1, 2, 18, 1, 18, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "I0726 03:35:18.147981 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 03:35:18.149189 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 1\n",
      "I0726 03:35:18.150211 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] He announced intentions to buy as much as one - billion - dollars worth of Argentine bonds and guaranteed Uruguay access to Venezuelan oil for decades . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 03:35:18.151228 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 1124 1717 11489 1106 4417 1112 1277 1112 1141 118 3775 118 5860 3869 1104 8106 10150 1105 13008 11752 2469 1106 16128 2949 1111 4397 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.152288 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.153059 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.153785 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'B-geo', 'O', 'O', 'B-gpe', 'O', 'O', 'O', 'O'] (ids = [19, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18, 18, 18, 18, 1, 1, 3, 1, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "I0726 03:35:18.155961 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 03:35:18.156719 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 2\n",
      "I0726 03:35:18.157493 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] Mr . Cha ##vez has been lobbying to join the South American trade block Me ##rc ##os ##ur , which is comprised of Argentina , Brazil , Paraguay , and Uruguay . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 03:35:18.158196 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 1828 119 24705 21755 1144 1151 22085 1106 2866 1103 1375 1237 2597 3510 2508 19878 2155 2149 117 1134 1110 11561 1104 4904 117 3524 117 14345 117 1105 11752 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.161662 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.162885 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 03:35:18.163549 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['B-per', 'B-geo', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-geo', 'O', 'B-geo', 'O', 'B-geo', 'O', 'O', 'B-geo', 'O'] (ids = [19, 4, 18, 2, 18, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 18, 18, 18, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "4796it [00:04, 995.41it/s] \n"
     ]
    }
   ],
   "source": [
    "eval_file = os.path.join('./workingBERTNERrisky', \"eval.tf_record\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(eval_file):\n",
    "    open(eval_file, 'w').close()\n",
    "\n",
    "eval_examples = create_examples(x_val)\n",
    "file_based_convert_examples_to_features(\n",
    "    eval_examples, MAX_SEQ_LENGTH, tokenizer, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0717 08:28:14.795304 140289763866368 estimator.py:1145] Calling model_fn.\n",
      "I0717 08:28:18.770100 140289763866368 estimator.py:1147] Done calling model_fn.\n",
      "I0717 08:28:18.789358 140289763866368 evaluation.py:255] Starting evaluation at 2019-07-17T08:28:18Z\n",
      "I0717 08:28:19.230360 140289763866368 monitored_session.py:240] Graph was finalized.\n",
      "I0717 08:28:19.248621 140289763866368 saver.py:1280] Restoring parameters from ./workingBERTNERrisky/output/model.ckpt-1348\n",
      "I0717 08:28:20.103464 140289763866368 session_manager.py:500] Running local_init_op.\n",
      "I0717 08:28:20.167800 140289763866368 session_manager.py:502] Done running local_init_op.\n",
      "I0717 08:29:02.748021 140289763866368 evaluation.py:275] Finished evaluation at 2019-07-17-08:29:02\n",
      "I0717 08:29:02.749196 140289763866368 estimator.py:2039] Saving dict for global step 1348: eval_accuracy = 0.99503005, false_negatives = 0.0, false_positives = 0.0, global_step = 1348, loss = 0.01687263, precision = 1.0, recall = 1.0, true_negatives = 484293.0, true_positives = 129595.0\n",
      "I0717 08:29:02.750421 140289763866368 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1348: ./workingBERTNERrisky/output/model.ckpt-1348\n"
     ]
    }
   ],
   "source": [
    "# This tells the estimator to run through the entire set.\n",
    "eval_steps = None\n",
    "\n",
    "eval_drop_remainder = False\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0717 08:29:08.858383 140289763866368 <ipython-input-64-3d6fa7ef3418>:3] ***** Eval results *****\n",
      "I0717 08:29:08.859773 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   eval_accuracy = 0.99503005\n",
      "I0717 08:29:08.861100 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   false_negatives = 0.0\n",
      "I0717 08:29:08.861933 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   false_positives = 0.0\n",
      "I0717 08:29:08.862548 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   global_step = 1348\n",
      "I0717 08:29:08.863182 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   loss = 0.01687263\n",
      "I0717 08:29:08.863795 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   precision = 1.0\n",
      "I0717 08:29:08.864350 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   recall = 1.0\n",
      "I0717 08:29:08.864983 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   true_negatives = 484293.0\n",
      "I0717 08:29:08.865566 140289763866368 <ipython-input-64-3d6fa7ef3418>:5]   true_positives = 129595.0\n"
     ]
    }
   ],
   "source": [
    "output_eval_file = os.path.join(\"./workingBERTNERrisky\", \"eval_results.txt\")\n",
    "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "    tf.logging.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPARE PREDICTIONS DATA ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,  max_seq_length, tokenizer, labelmap=label_map, pretokenized=False):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    features = []\n",
    "    origin_tokens = []\n",
    "    for (ex_index, example) in tqdm(enumerate(examples)):\n",
    "        feature, origin_token = convert_single_example(ex_index, example, max_seq_length, tokenizer, pretokenized=pretokenized)\n",
    "        features.append(feature)\n",
    "        origin_tokens.append(origin_token)\n",
    "        \n",
    "    return features, origin_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_ids)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    \n",
    "    #normally this would be the number of \n",
    "    num_examples = len(features)\n",
    "\n",
    "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples, seq_length], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in label_map.items()}\n",
    "def getPredictionDf(dataframe):\n",
    "    #assuming dataframe of which first column is ['Word'] and each row contains a comma seperated list of words [\"This\",\"is\",\"my\",\"sentence\",\".\"]\n",
    "    labels = LABELS\n",
    "    sentences = dataframe['Word'].tolist()\n",
    "    predict_examples = create_examples(dataframe, False)\n",
    "    predict_features, predict_tokens = convert_examples_to_features(predict_examples, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = input_fn_builder(features=predict_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    outputs = []\n",
    "    for sentence,token_indexes,predicto in zip(sentences, predict_tokens, predictions):\n",
    "        retdict = {}\n",
    "        retdict['sentence'] = sentence\n",
    "        predict_array = predicto['labels']\n",
    "        endlabels = []\n",
    "        for word, tok_index in zip(sentence,token_indexes):\n",
    "            label_text = inv_map[predict_array[tok_index]]\n",
    "            endlabels.append(label_text)\n",
    "        retdict['labels'] = endlabels\n",
    "        outputs.append(retdict)\n",
    "    #return [(token, prediction['probabilities'], labels[prediction['labels']]) for token, prediction in zip(predict_tokens,predictions)]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "def simpletokenizer(text):\n",
    "    text = text.replace('\\n',' \\n ')\n",
    "    text = text.replace('. ', ' . ')\n",
    "    text = re.sub('\\.([A-Z])\\s\\.', r'.\\1.', text)\n",
    "    puncnostop = string.punctuation.replace('.','')\n",
    "    text = text.translate(str.maketrans({key: \" {0} \".format(key) for key in puncnostop}))\n",
    "    text = tf.keras.preprocessing.text.text_to_word_sequence(text, filters='', lower=False, split=' ')\n",
    "    return(text)\n",
    "\n",
    "def grouper(simpletokens,length,dataframe):\n",
    "    for i in range(0,len(simpletokens), length):\n",
    "        batch = simpletokens[i:i+length]\n",
    "        while len(batch) < MAX_SEQ_LENGTH:\n",
    "            batch.append('[PAD]')\n",
    "        dataframe.append({'Word':batch},ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "def convertDocStringtoTokens(docstring):\n",
    "    #takes a block of text (string) of any length (ie an entire document) and returns word level tokens, BERTtokens as a df and an index linking the two\n",
    "    labels = LABELS\n",
    "    #pad punctuation with whitespace then split on whitespace\n",
    "    #we will use these tokens to rebuild our marked up version of the text\n",
    "    wordtokens = simpletokenizer(docstring)\n",
    "    #this map keeps track of the index of the original token compared to the BERT token\n",
    "    word_to_tok_map = []\n",
    "    BERTparentlabels = []\n",
    "    BERTparent = []\n",
    "    BERTlabels = []\n",
    "    BERTtokens = []\n",
    "    x1 = 0\n",
    "    y1 = 0\n",
    "    for word in wordtokens:\n",
    "        #add the special tags to begin and end sequences\n",
    "        #we stop 10 short of the max sequence length as we don't want to risk splitting tokens for a single entity across input sequences\n",
    "        if x1 >= (MAX_SEQ_LENGTH-10):\n",
    "            BERTtokens.append('[SEP]')\n",
    "            BERTlabels.append('[SEP]')            \n",
    "            while len(BERTtokens)<128:\n",
    "                #PAD out the remainder of the sequence up to 127 tokens then add the final 'sep'\n",
    "                BERTtokens.append('[PAD]')\n",
    "                BERTlabels.append('[PAD]')          \n",
    "            BERTparent.append(BERTtokens)\n",
    "            BERTparentlabels.append(BERTlabels)\n",
    "            BERTtokens=[]\n",
    "            BERTlabels=[]\n",
    "            x1 = 0\n",
    "            y1 += 128\n",
    "        if x1 == 0:\n",
    "            BERTtokens.append('[CLS]')\n",
    "            BERTlabels.append('[CLS]')\n",
    "            x1 = 1\n",
    "        word_to_tok_map.append(len(BERTtokens)+y1)\n",
    "        #tokenise using the BERT tokenizer (running = 'run','##ing')\n",
    "        #remember tokenizer returns an array, even if we give it one word\n",
    "        sub_words = tokenizer.tokenize(word)\n",
    "        extras = ['X']*(len(sub_words)-1)\n",
    "        BERTtokens.extend(sub_words)\n",
    "        BERTlabels.append('[PAD]')\n",
    "        BERTlabels.extend(extras)\n",
    "        x1 += len(sub_words)\n",
    "    if len(BERTtokens)!=128:\n",
    "        BERTtokens.append('[SEP]')\n",
    "        BERTlabels.append('[SEP]')\n",
    "    while len(BERTtokens)<128:\n",
    "        #PAD out the remainder of the sequence up to 127 tokens then add the final 'sep'\n",
    "        BERTtokens.append('[PAD]')\n",
    "        BERTlabels.append('[PAD]')    \n",
    "    BERTparent.append(BERTtokens)\n",
    "    BERTparentlabels.append(BERTlabels)\n",
    "    dataframe = pd.DataFrame({'Labels':BERTparentlabels})\n",
    "    dataframe.insert(0,'Word',BERTparent)\n",
    "    return wordtokens, word_to_tok_map, dataframe\n",
    "    \n",
    "def getPredictionDocString(docstring):\n",
    "    #takes a block of text (string) of any length (ie an entire document) and returns marked up text and entity list\n",
    "    wordtokens, word_to_tok_map, dataframe = convertDocStringtoTokens(docstring)\n",
    "    labels = LABELS\n",
    "    predict_examples = create_examples(dataframe, False, True)\n",
    "    predict_features, predict_tokens = convert_examples_to_features(predict_examples, MAX_SEQ_LENGTH, tokenizer,pretokenized=True)\n",
    "    predict_input_fn = input_fn_builder(features=predict_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    outputs = []\n",
    "    #rejoin all the predictions for each sequence as though they were one document\n",
    "    allpredictions = []\n",
    "    for predicto in predictions:\n",
    "        predict_array = predicto['labels']\n",
    "        for labelint in predict_array:\n",
    "            allpredictions.append(inv_map[labelint])\n",
    "    entitylist = []\n",
    "    labellist = []\n",
    "    entitydict ={}\n",
    "    markup = ''\n",
    "    #This next bit is kind of herendous but necessary to stitch back together words to labels, rather than BERT tokens to labels\n",
    "    #We also construct the marked up text by rejoining words with whitespace and spans with the label class\n",
    "    for word,token_index,predicted in zip(wordtokens, word_to_tok_map, allpredictions):\n",
    "        wordlabel = allpredictions[token_index]\n",
    "        if wordlabel not in ['O','X','[SEP]','[PAD]','[CLS]']:\n",
    "            if wordlabel[0] == 'I':\n",
    "                joinedentity = entitylist.pop() + ' ' + word\n",
    "                entitylist.append(joinedentity)\n",
    "                markup = markup[:-7] + ' ' + word + '</span>'\n",
    "            else:\n",
    "                if word == '\\n':\n",
    "                    markup = markup + word\n",
    "                else:    \n",
    "                    entitylist.append(word)\n",
    "                    labellist.append(wordlabel)\n",
    "                    markup = markup + ' <span class=\"' + wordlabel.lower() + '\">'+word+'</span>'\n",
    "        elif wordlabel not in ['[SEP]','[PAD]','[CLS]']:\n",
    "            if word in string.punctuation:\n",
    "                if word in ['.','”']:\n",
    "                    markup = markup + '. '\n",
    "                elif word == '“':\n",
    "                    markup = markup + ' \"'\n",
    "                else:\n",
    "                    markup = markup + word\n",
    "            else:\n",
    "                markup = markup + ' ' + word\n",
    "                \n",
    "    for entity,label in zip(entitylist, labellist):\n",
    "        entitydict[entity] = label\n",
    "    return entitydict, markup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAKE A PREDICTION ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... using a document string ###\n",
    "\n",
    "Note this is assuming one long document and there is a slow spin-up time each tensorflow calls estimator.predict(). Estimator.predict() causes the model to save its state before running. \n",
    "\n",
    "To classify a batch of documents efficiently you must adjust this code to call predict once or infrequently by sending the 128-length sequences for all docs together as a big batch, then later splitting them apart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdoc = 'Despite criticism from Democrats that his comments about the four minority congresswomen are racist, Trump went on an extended diatribe about the lawmakers, saying they were welcome to leave the country if they did not like his policies on issues such as immigration and defending Israel.\\n“So these Congresswomen, their comments are helping to fuel the rise of a dangerous, militant hard left,” the Republican president said to roars from the crowd in North Carolina, a state seen as key to his re-election.\\nTrump tweeted over the weekend that the four progressive representatives, known as “the squad” - Ilhan Omar of Minnesota, Alexandria Ocasio-Cortez of New York, Rashida Tlaib of Michigan and Ayanna Pressley of Massachusetts - should “go back” where they came from, even though all are U.S. citizens and three are U.S.-born.\\nThe aim, one source close to Trump said, was to make Democrats look as far left as possible to moderate voters as he girds for a tough re-election battle in November 2020.\\n“He is trying to make them the face of the Democratic Party as we move closer into the 2020 cycle and he’s trying to highlight them as a fringe crowd as much as possible so it turns off your middle-of-the-road voters,” the source said.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:00, 8366.30it/s]\n",
      "0it [00:00, ?it/s]I0726 06:18:08.623424 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 06:18:08.624508 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 0\n",
      "I0726 06:18:08.625475 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] Despite criticism from Democrats that his comments about the four minority congress ##wo ##men are racist , Trump went on an extended di ##at ##ri ##be about the law ##makers , saying they were welcome to leave the country if they did not like his policies on issues such as immigration and defending Israel . “ So these Congress ##wo ##men , their comments are helping to fuel the rise of a dangerous , militant hard left , ” the Republican president said to roar ##s from the crowd in North Carolina , a state seen as key to his re - election . Trump t ##weet ##ed over the weekend that the four progressive representatives , [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 06:18:08.627011 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 2711 5879 1121 7834 1115 1117 7640 1164 1103 1300 7309 16821 12821 2354 1132 18848 117 8499 1355 1113 1126 2925 4267 2980 2047 3962 1164 1103 1644 11877 117 2157 1152 1127 7236 1106 1817 1103 1583 1191 1152 1225 1136 1176 1117 5502 1113 2492 1216 1112 9027 1105 6611 3103 119 789 1573 1292 2757 12821 2354 117 1147 7640 1132 4395 1106 4251 1103 3606 1104 170 4249 117 19850 1662 1286 117 790 1103 3215 2084 1163 1106 14148 1116 1121 1103 3515 1107 1456 2938 117 170 1352 1562 1112 2501 1106 1117 1231 118 1728 119 8499 189 24887 1174 1166 1103 5138 1115 1103 1300 8706 6683 117 102 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.628021 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0726 06:18:08.628936 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.630033 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['[CLS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] (ids = [19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "I0726 06:18:08.632691 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 06:18:08.633551 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 1\n",
      "I0726 06:18:08.634404 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] known as “ the squad ” - Il ##han Omar of Minnesota , Alexandria O ##cas ##io - Co ##rte ##z of New York , Rashid ##a T ##lai ##b of Michigan and A ##yan ##na Press ##ley of Massachusetts - should “ go back ” where they came from , even though all are U . S . citizens and three are U . S . - born . The aim , one source close to Trump said , was to make Democrats look as far left as possible to moderate voters as he g ##ir ##ds for a tough re - election battle in November 2020 . “ He is trying to make them the [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 06:18:08.636108 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 1227 1112 789 1103 4322 790 118 9190 3822 13569 1104 4332 117 10142 152 21995 2660 118 3291 22460 1584 1104 1203 1365 117 24736 1161 157 20737 1830 1104 3312 1105 138 6582 1605 2544 1926 1104 3559 118 1431 789 1301 1171 790 1187 1152 1338 1121 117 1256 1463 1155 1132 158 119 156 119 4037 1105 1210 1132 158 119 156 119 118 1255 119 1109 6457 117 1141 2674 1601 1106 8499 1163 117 1108 1106 1294 7834 1440 1112 1677 1286 1112 1936 1106 8828 7179 1112 1119 176 3161 3680 1111 170 8035 1231 118 1728 2321 1107 1379 12795 119 789 1124 1110 1774 1106 1294 1172 1103 102 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.636980 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0726 06:18:08.637928 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.638800 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['[CLS]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', 'X', '[PAD]', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] (ids = [19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "I0726 06:18:08.639850 139806183552768 <ipython-input-19-5c09d3e76962>:130] *** Example ***\n",
      "I0726 06:18:08.640642 139806183552768 <ipython-input-19-5c09d3e76962>:131] guid: 2\n",
      "I0726 06:18:08.642680 139806183552768 <ipython-input-19-5c09d3e76962>:133] tokens: [CLS] face of the Democratic Party as we move closer into the 2020 cycle and he ’ s trying to highlight them as a fringe crowd as much as possible so it turns off your middle - of - the - road voters , ” the source said . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "I0726 06:18:08.643515 139806183552768 <ipython-input-19-5c09d3e76962>:134] input_ids: 101 1339 1104 1103 2978 1786 1112 1195 1815 2739 1154 1103 12795 5120 1105 1119 787 188 1774 1106 13426 1172 1112 170 22164 3515 1112 1277 1112 1936 1177 1122 3587 1228 1240 2243 118 1104 118 1103 118 1812 7179 117 790 1103 2674 1163 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.644422 139806183552768 <ipython-input-19-5c09d3e76962>:135] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "I0726 06:18:08.645768 139806183552768 <ipython-input-19-5c09d3e76962>:136] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I0726 06:18:08.646581 139806183552768 <ipython-input-19-5c09d3e76962>:137] label: ['[CLS]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', 'X', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', 'X', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] (ids = [19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "3it [00:00, 118.70it/s]\n",
      "I0726 06:18:08.671784 139806183552768 estimator.py:1145] Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of tokens is  128\n",
      "['[CLS]', 'Despite', 'criticism', 'from', 'Democrats', 'that', 'his', 'comments', 'about', 'the', 'four', 'minority', 'congress', '##wo', '##men', 'are', 'racist', ',', 'Trump', 'went', 'on', 'an', 'extended', 'di', '##at', '##ri', '##be', 'about', 'the', 'law', '##makers', ',', 'saying', 'they', 'were', 'welcome', 'to', 'leave', 'the', 'country', 'if', 'they', 'did', 'not', 'like', 'his', 'policies', 'on', 'issues', 'such', 'as', 'immigration', 'and', 'defending', 'Israel', '.', '“', 'So', 'these', 'Congress', '##wo', '##men', ',', 'their', 'comments', 'are', 'helping', 'to', 'fuel', 'the', 'rise', 'of', 'a', 'dangerous', ',', 'militant', 'hard', 'left', ',', '”', 'the', 'Republican', 'president', 'said', 'to', 'roar', '##s', 'from', 'the', 'crowd', 'in', 'North', 'Carolina', ',', 'a', 'state', 'seen', 'as', 'key', 'to', 'his', 're', '-', 'election', '.', 'Trump', 't', '##weet', '##ed', 'over', 'the', 'weekend', 'that', 'the', 'four', 'progressive', 'representatives', ',', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "length of tokens is  128\n",
      "['[CLS]', 'known', 'as', '“', 'the', 'squad', '”', '-', 'Il', '##han', 'Omar', 'of', 'Minnesota', ',', 'Alexandria', 'O', '##cas', '##io', '-', 'Co', '##rte', '##z', 'of', 'New', 'York', ',', 'Rashid', '##a', 'T', '##lai', '##b', 'of', 'Michigan', 'and', 'A', '##yan', '##na', 'Press', '##ley', 'of', 'Massachusetts', '-', 'should', '“', 'go', 'back', '”', 'where', 'they', 'came', 'from', ',', 'even', 'though', 'all', 'are', 'U', '.', 'S', '.', 'citizens', 'and', 'three', 'are', 'U', '.', 'S', '.', '-', 'born', '.', 'The', 'aim', ',', 'one', 'source', 'close', 'to', 'Trump', 'said', ',', 'was', 'to', 'make', 'Democrats', 'look', 'as', 'far', 'left', 'as', 'possible', 'to', 'moderate', 'voters', 'as', 'he', 'g', '##ir', '##ds', 'for', 'a', 'tough', 're', '-', 'election', 'battle', 'in', 'November', '2020', '.', '“', 'He', 'is', 'trying', 'to', 'make', 'them', 'the', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "length of tokens is  128\n",
      "['[CLS]', 'face', 'of', 'the', 'Democratic', 'Party', 'as', 'we', 'move', 'closer', 'into', 'the', '2020', 'cycle', 'and', 'he', '’', 's', 'trying', 'to', 'highlight', 'them', 'as', 'a', 'fringe', 'crowd', 'as', 'much', 'as', 'possible', 'so', 'it', 'turns', 'off', 'your', 'middle', '-', 'of', '-', 'the', '-', 'road', 'voters', ',', '”', 'the', 'source', 'said', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0726 06:18:11.704499 139806183552768 estimator.py:1147] Done calling model_fn.\n",
      "I0726 06:18:12.228865 139806183552768 monitored_session.py:240] Graph was finalized.\n",
      "I0726 06:18:12.232276 139806183552768 saver.py:1280] Restoring parameters from ./workingBERTNERrisky/output/model.ckpt-1348\n",
      "I0726 06:18:13.430268 139806183552768 session_manager.py:500] Running local_init_op.\n",
      "I0726 06:18:13.492712 139806183552768 session_manager.py:502] Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alexandria Ocasio': 'B-geo', 'November 2020': 'B-tim', 'Michigan': 'B-geo', 'Massachusetts': 'B-geo', 'Democratic Party': 'B-org', 'New York': 'B-geo', 'Minnesota': 'B-geo', 'Israel': 'B-geo', 'Trump': 'B-per', 'Rashida Tlaib': 'B-per', 'Congresswomen': 'B-org', 'North Carolina': 'B-geo', 'weekend': 'B-tim', 'Ilhan Omar': 'B-per', 'Ayanna Pressley': 'B-per', 'U.S.': 'B-geo', '2020': 'B-tim'}\n",
      " Despite criticism from Democrats that his comments about the four minority congresswomen are racist, <span class=\"b-per\">Trump</span> went on an extended diatribe about the lawmakers, saying they were welcome to leave the country if they did not like his policies on issues such as immigration and defending <span class=\"b-geo\">Israel</span>.  these <span class=\"b-org\">Congresswomen</span>, their comments are helping to fuel the rise of a dangerous, militant hard left, ” the Republican president said to roars from the crowd in <span class=\"b-geo\">North Carolina</span>, a state seen as key to his re- election. \n",
      " <span class=\"b-per\">Trump</span> tweeted over the <span class=\"b-tim\">weekend</span> that the four progressive representatives, known as “the squad”- <span class=\"b-per\">Ilhan Omar</span> of <span class=\"b-geo\">Minnesota</span>, <span class=\"b-geo\">Alexandria Ocasio</span>- Cortez of <span class=\"b-geo\">New York</span>, <span class=\"b-per\">Rashida Tlaib</span> of <span class=\"b-geo\">Michigan</span> and <span class=\"b-per\">Ayanna Pressley</span> of <span class=\"b-geo\">Massachusetts</span>- should “go back” where they came from, even though all are <span class=\"b-geo\">U.S.</span> citizens and three are U.S.- born.  \n",
      " The aim, one source close to <span class=\"b-per\">Trump</span> said, was to make Democrats look as far left as possible to moderate voters as he girds for a tough re- election battle in <span class=\"b-tim\">November 2020</span>.  \n",
      " “He is trying to make them the face of the <span class=\"b-org\">Democratic Party</span> as we move closer into the <span class=\"b-tim\">2020</span> cycle and he’s trying to highlight them as a fringe crowd as much as possible so it turns off your middle- of- the- road voters, ” the source said.\n"
     ]
    }
   ],
   "source": [
    "entities, markup = getPredictionDocString(testdoc)\n",
    "print(entities)\n",
    "print(markup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
